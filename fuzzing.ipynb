{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3fe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST, SVHN\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "# from feature_wise_pgd import PGD\n",
    "from resnet20 import ResNet20\n",
    "from LeNet import LeNet5\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771a01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# test_dataset=TensorDataset(x_test,y_test)\n",
    "train_dataset = SVHN(root='../data', split='train', transform=ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset = 'schn'\n",
    "model = 'lenet5'\n",
    "testmodel = torch.load('./pretrained/svhn_lenet5_0.842.pt')\n",
    "VS_idx = np.load('./deepfeature/'+ dataset + '-' + model +'-VS-5.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9738bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGD():\n",
    "    def __init__(self, model, eps=8/255,\n",
    "                 alpha=2/255, steps=3, random_start=True):\n",
    "        self.model = model.cuda()\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.random_start = random_start\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "        self._targeted = False\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "    def forward(self, images, labels, VS):\n",
    "\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        adv_images = images.clone().detach()\n",
    "\n",
    "        _, clean_feature_map = self.model(images)\n",
    "        # print(clean_feature_map.shape)\n",
    "        cost_array = []\n",
    "        adv_feature_map_array = []\n",
    "        adv_images_array = []\n",
    "\n",
    "\n",
    "        self.fuzz_images = images.clone().cuda()\n",
    "        self.fuzz_labels = labels.clone().cuda()\n",
    "        self.fuzz_outputs = labels.clone().cuda()\n",
    "        start_time = time.time()\n",
    "        while time.time()-start_time < 1:\n",
    "            adv_images = images.clone().detach()\n",
    "            if self.random_start:\n",
    "                # Starting at a uniformly random point\n",
    "                adv_images = adv_images + torch.empty_like(adv_images).uniform_(-self.eps, self.eps)\n",
    "                adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
    "            for _ in range(self.steps):\n",
    "                adv_images.requires_grad = True\n",
    "                outputs, adv_feature_map = self.model(adv_images)\n",
    "\n",
    "\n",
    "                # Calculate loss\n",
    "                # print(adv_feature_map[:,idx].shape)\n",
    "                # cost = nn.functional.cosine_similarity(torch.flatten(adv_feature_map[:,idx],   1)\n",
    "                                                    #   ,torch.flatten(clean_feature_map[:,idx], 1), dim=0)\n",
    "                # cost = cost.mean()\n",
    "                cost = loss(adv_feature_map[:,VS], clean_feature_map[:,VS])\n",
    "                # cost = loss(outputs, labels)\n",
    "\n",
    "                # Update adversarial images\n",
    "                grad = torch.autograd.grad(cost, adv_images,\n",
    "                                        retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "                adv_images = adv_images.detach() + self.alpha * grad.sign()\n",
    "                delta = torch.clamp(adv_images - images, min=-self.eps, max=self.eps)\n",
    "                adv_images = torch.clamp(images + delta, min=0, max=1).detach()\n",
    "                self.fuzz_images = torch.cat((self.fuzz_images,adv_images),0)\n",
    "                self.fuzz_labels = torch.cat((self.fuzz_labels,labels),0)\n",
    "                self.fuzz_outputs = torch.cat((self.fuzz_outputs,torch.argmax(outputs,dim=1)),0)\n",
    "\n",
    "        return self.fuzz_images[1:], self.fuzz_labels[1:], self.fuzz_outputs[1:]\n",
    "\n",
    "#             cost_array.append(cost.cpu().detach())\n",
    "#             outputs, adv_feature_map = self.model(adv_images)\n",
    "#             adv_images_array.append(adv_images)\n",
    "#             adv_feature_map_array.append(adv_feature_map[:,idx])\n",
    "            \n",
    "\n",
    "#         cost_array = np.array(cost_array)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6323db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  7 14 13  2  9  1  4 11  0]\n",
      "321\n",
      "336\n",
      "330\n",
      "330\n",
      "333\n",
      "336\n",
      "351\n",
      "354\n",
      "357\n",
      "360\n",
      "354\n",
      "354\n",
      "357\n",
      "351\n",
      "354\n",
      "357\n",
      "372\n",
      "423\n",
      "357\n",
      "351\n",
      "363\n",
      "357\n",
      "360\n",
      "363\n",
      "363\n",
      "357\n",
      "357\n",
      "363\n",
      "369\n",
      "360\n",
      "360\n",
      "363\n",
      "366\n",
      "360\n",
      "366\n",
      "360\n",
      "363\n",
      "360\n",
      "363\n",
      "357\n",
      "363\n",
      "360\n",
      "363\n",
      "363\n",
      "363\n",
      "360\n",
      "363\n",
      "366\n",
      "363\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "357\n",
      "363\n",
      "360\n",
      "360\n",
      "357\n",
      "363\n",
      "366\n",
      "363\n",
      "363\n",
      "357\n",
      "366\n",
      "360\n",
      "360\n",
      "360\n",
      "363\n",
      "363\n",
      "363\n",
      "363\n",
      "366\n",
      "360\n",
      "363\n",
      "360\n",
      "369\n",
      "360\n",
      "360\n",
      "360\n",
      "366\n",
      "366\n",
      "360\n",
      "360\n",
      "366\n",
      "363\n",
      "363\n",
      "360\n",
      "366\n",
      "360\n",
      "360\n",
      "357\n",
      "363\n",
      "363\n",
      "366\n",
      "360\n",
      "363\n",
      "363\n",
      "360\n",
      "360\n",
      "366\n",
      "360\n",
      "363\n",
      "363\n",
      "369\n",
      "399\n",
      "384\n",
      "366\n",
      "372\n",
      "363\n",
      "366\n",
      "360\n",
      "366\n",
      "357\n",
      "357\n",
      "360\n",
      "366\n",
      "360\n",
      "357\n",
      "360\n",
      "363\n",
      "360\n",
      "360\n",
      "357\n",
      "366\n",
      "360\n",
      "360\n",
      "360\n",
      "366\n",
      "357\n",
      "360\n",
      "360\n",
      "366\n",
      "357\n",
      "363\n",
      "357\n",
      "360\n",
      "360\n",
      "360\n",
      "363\n",
      "366\n",
      "360\n",
      "360\n",
      "360\n",
      "366\n",
      "360\n",
      "360\n",
      "369\n",
      "360\n",
      "360\n",
      "360\n",
      "363\n",
      "363\n",
      "360\n",
      "369\n",
      "363\n",
      "363\n",
      "360\n",
      "360\n",
      "366\n",
      "357\n",
      "360\n",
      "357\n",
      "360\n",
      "357\n",
      "357\n",
      "360\n",
      "363\n",
      "360\n",
      "357\n",
      "357\n",
      "363\n",
      "360\n",
      "357\n",
      "357\n",
      "360\n",
      "357\n",
      "354\n",
      "360\n",
      "366\n",
      "363\n",
      "360\n",
      "363\n",
      "369\n",
      "420\n",
      "480\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "480\n",
      "483\n",
      "483\n",
      "483\n",
      "426\n",
      "420\n",
      "414\n",
      "372\n",
      "363\n",
      "360\n",
      "363\n",
      "360\n",
      "360\n",
      "369\n",
      "360\n",
      "363\n",
      "360\n",
      "369\n",
      "360\n",
      "360\n",
      "363\n",
      "369\n",
      "360\n",
      "363\n",
      "363\n",
      "369\n",
      "363\n",
      "360\n",
      "363\n",
      "378\n",
      "378\n",
      "360\n",
      "360\n",
      "363\n",
      "360\n",
      "357\n",
      "357\n",
      "363\n",
      "357\n",
      "360\n",
      "360\n",
      "366\n",
      "360\n",
      "360\n",
      "363\n",
      "366\n",
      "360\n",
      "357\n",
      "360\n",
      "360\n",
      "354\n",
      "360\n",
      "360\n",
      "357\n",
      "363\n",
      "360\n",
      "360\n",
      "357\n",
      "357\n",
      "357\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "attack = PGD(testmodel, eps=0.1, alpha=0.1/2, steps=3, random_start=True)\n",
    "start_time = time.time()\n",
    "iterator=iter(train_loader)\n",
    "image, label = next(iterator)\n",
    "fuzz_images = image.clone().cuda()\n",
    "fuzz_labels = label.clone().cuda()\n",
    "truth_label = label.clone().cuda()\n",
    "print(VS_idx)\n",
    "for _, (image, label) in enumerate(train_loader):\n",
    "\n",
    "    fuzzer_images,ground_truth, fuzzer_labels = attack.forward(image,label,VS_idx)\n",
    "    fuzz_images = torch.cat((fuzz_images,fuzzer_images),0)\n",
    "    fuzz_labels = torch.cat((fuzz_labels,fuzzer_labels),0)\n",
    "    truth_label = torch.cat((truth_label,ground_truth),0)\n",
    "    print(len(fuzzer_images))\n",
    "    if time.time()-start_time>300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43521e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fuzz_images),len(fuzz_labels))\n",
    "# fuzz_labels = np.array(fuzz_labels)\n",
    "# fuzz_images = np.array(fuzz_images)\n",
    "# fuzz_x = np.concatenate(fuzz_images,axis = 0)\n",
    "# fuzz_x = np.squeeze(fuzz_x,axis = 1)\n",
    "print(fuzz_images.shape)\n",
    "# fuzz_predict = np.concatenate(fuzz_labels,axis = 0)\n",
    "print(fuzz_labels.shape)\n",
    "# fuzz_predict = np.squeeze(fuzz_predict,axis = 1)\n",
    "# print(fuzz_predict.shape)\n",
    "# print(ground_truth.shape)\n",
    "# fuzz_y = np.array(truth_label)\n",
    "# fuzz_y = np.concatenate(truth_label,axis = 0)\n",
    "print(truth_label.shape)\n",
    "\n",
    "correct = truth_label.eq(fuzz_labels).sum().item()\n",
    "# FDR = np.sum((fuzz_labels==truth_label))\n",
    "fdd = {}\n",
    "misclassfy_x = []\n",
    "misclassfy_y = []\n",
    "misclassify_idx = [] \n",
    "for i in range(len(fuzz_images)):\n",
    "    if fuzz_labels[i]!=truth_label[i]:\n",
    "        fdd[(str(fuzz_labels[i]),str(truth_label[i]))] = 1\n",
    "        misclassify_idx.append(i)\n",
    "print('Fuzzing number is:', correct)\n",
    "print('Fuzzing fault type is:', len(fdd))\n",
    "misclassfy_x = fuzz_images[misclassify_idx]\n",
    "misclassfy_y = truth_label[misclassify_idx]\n",
    "# np.save('./fuzz_cifar10_vgg16_images.npy', misclassfy_x.cpu().numpy())\n",
    "# np.save('./fuzz_cifar10_vgg16_labels.npy', misclassfy_y.cpu().numpy())\n",
    "print(misclassfy_x.shape)\n",
    "print(misclassfy_y.shape)\n",
    "# misclassfy=TensorDataset(misclassfy_x,misclassfy_y)\n",
    "# torch.save(misclassfy, './rq5/cifar10_vgg16_fuzz.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d665e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader, optimizer):\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for idx in range(2):\n",
    "        with tqdm(train_loader[idx]) as loader:\n",
    "            for data, target in loader:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output,_ = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader):\n",
    "        test_x = test_x.cuda()\n",
    "        predict_y, _ = model(test_x.float())\n",
    "        predict_y = np.argmax(predict_y.cpu().detach(), axis=-1)\n",
    "        # print(predict_y.shape)\n",
    "        current_correct_num = predict_y == test_label\n",
    "        all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "    print('accuracy: {:.5f}'.format(acc))\n",
    "#         scheduler.step()\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "testmodel = torch.load('./pretrained/svhn_lenet5_0.842.pt')\n",
    "random_idx = np.random.choice(np.linspace(0, 37724, 37724, dtype=np.int32), 11273)\n",
    "print(misclassfy_x.shape)\n",
    "train_dataset=TensorDataset(misclassfy_x[torch.from_numpy(random_idx).type(torch.long)],misclassfy_y[torch.from_numpy(random_idx).type(torch.long)])\n",
    "train_dataset_ori = SVHN(root='../data', split='train', transform=ToTensor(), download=True)\n",
    "test_dataset = SVHN(root='../data', split='test', transform=ToTensor(), download=True)\n",
    "train_loader_ori = DataLoader(train_dataset_ori, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "optimizer = torch.optim.SGD(testmodel.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [14])\n",
    "best_acc = 0\n",
    "for e in range(20):\n",
    "    print('Epoch: ', e+1)\n",
    "    acc = train(testmodel, (train_loader, train_loader_ori), test_loader, optimizer)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model, './rq5/svhn_lenet5_best.pt')\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68482da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6cb53a8fcb1f79e68e45a16d1911a262298e134f1af2525e667b58dfa31422f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
